{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3bZi1yTs+DOb6UXzRy2bS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"q85tbPEJrtif"},"outputs":[],"source":["%pip install llama-index-vector-stores-lancedb\n","%pip install llama-index-multi-modal-llms-openai"]},{"cell_type":"code","source":["%pip install llama-index-multi-modal-llms-openai\n","%pip install llama-index-vector-stores-lancedb\n","%pip install llama-index-embeddings-clip"],"metadata":{"id":"KlKYy38Zrwkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install llama_index ftfy regex tqdm\n","%pip install -U openai-whisper\n","%pip install git+https://github.com/openai/CLIP.git\n","%pip install torch torchvision\n","%pip install matplotlib scikit-image\n","%pip install lancedb\n","%pip install moviepy\n","%pip install pytube\n","%pip install pydub\n","%pip install SpeechRecognition\n","%pip install ffmpeg-python\n","%pip install soundfile"],"metadata":{"id":"NAMjzfZyr1Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from moviepy.editor import VideoFileClip\n","from pathlib import Path\n","import speech_recognition as sr\n","from pytube import YouTube\n","from pprint import pprint"],"metadata":{"id":"GVx1H2iPsaeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = '<YOUR_API_KEY>'"],"metadata":{"id":"L12mtRSLtpkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["video_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n","output_video_path = \"./video_data/\"\n","output_folder = \"./mixed_data/\"\n","output_audio_path = \"./mixed_data/output_audio.wav\"\n","\n","filepath = output_video_path + \"input_vid.mp4\"\n","Path(output_folder).mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"X5iTUknBtqkT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","\n","#추후 image retrieval 결과 플라팅 용\n","def plot_images(image_paths):\n","    images_shown = 0\n","    plt.figure(figsize=(16, 9))\n","    for img_path in image_paths:\n","        if os.path.isfile(img_path):\n","            image = Image.open(img_path)\n","\n","            plt.subplot(2, 3, images_shown + 1)\n","            plt.imshow(image)\n","            plt.xticks([])\n","            plt.yticks([])\n","\n","            images_shown += 1\n","            if images_shown >= 7:\n","                break"],"metadata":{"id":"H57bFaHFtugw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_video(url, output_path):\n","    yt = YouTube(url)\n","    metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": yt.views}\n","    yt.streams.get_highest_resolution().download(\n","        output_path=output_path, filename=\"input_vid.mp4\"\n","    )\n","    return metadata\n","\n","\n","def video_to_images(video_path, output_folder):\n","    clip = VideoFileClip(video_path)\n","    clip.write_images_sequence(\n","        os.path.join(output_folder, \"frame%04d.png\"), fps=0.2\n","    )\n","\n","\n","def video_to_audio(video_path, output_audio_path):\n","    clip = VideoFileClip(video_path)\n","    audio = clip.audio\n","    audio.write_audiofile(output_audio_path)\n","\n","\n","def audio_to_text(audio_path):\n","    recognizer = sr.Recognizer()\n","    audio = sr.AudioFile(audio_path)\n","\n","    with audio as source:\n","        audio_data = recognizer.record(source)\n","\n","        try:\n","            text = recognizer.recognize_whisper(audio_data)\n","        except sr.UnknownValueError:\n","            print(\"Speech recognition could not understand the audio.\")\n","        except sr.RequestError as e:\n","            print(f\"Could not request results from service; {e}\")\n","\n","    return text"],"metadata":{"id":"xLX2-Pxytx6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    metadata_vid = download_video(video_url, output_video_path)\n","    video_to_images(filepath, output_folder)\n","    video_to_audio(filepath, output_audio_path)\n","    text_data = audio_to_text(output_audio_path)\n","\n","    with open(output_folder + \"output_text.txt\", \"w\") as file:\n","        file.write(text_data)\n","    print(\"Text data saved to file\")\n","    file.close()\n","    os.remove(output_audio_path)\n","    print(\"Audio file removed\")\n","\n","except Exception as e:\n","    raise e"],"metadata":{"id":"QNJ5yIa-t45j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from llama_index.core.indices import MultiModalVectorStoreIndex\n","from llama_index.core import SimpleDirectoryReader, StorageContext\n","\n","from llama_index.core import SimpleDirectoryReader, StorageContext\n","from llama_index.vector_stores.lancedb import LanceDBVectorStore\n","\n","\n","from llama_index.core import SimpleDirectoryReader\n","\n","text_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"text_collection\")\n","image_store = LanceDBVectorStore(uri=\"lancedb\", table_name=\"image_collection\")\n","storage_context = StorageContext.from_defaults(\n","    vector_store=text_store, image_store=image_store\n",")\n","\n","# -> OpenAI CLIP 모델로 Text, Image Embedding 가해짐\n","documents = SimpleDirectoryReader(output_folder).load_data()\n","\n","index = MultiModalVectorStoreIndex.from_documents(\n","    documents,\n","    storage_context=storage_context,\n",")"],"metadata":{"id":"vxxnFpEvt6Sv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d_ZVk8Gkv891"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_engine = index.as_retriever(\n","    similarity_top_k=5, image_similarity_top_k=5\n",")"],"metadata":{"id":"ZCa_Xh6Pvlzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import json\n","\n","metadata_str = json.dumps(metadata_vid)\n","\n","qa_tmpl_str = (\n","    \"Given the provided information, including relevant images and retrieved context from the video, \\\n"," accurately and precisely answer the query without any additional prior knowledge.\\n\"\n","    \"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\n","    \"---------------------\\n\"\n","    \"Context: {context_str}\\n\"\n","    \"Metadata for video: {metadata_str} \\n\"\n","    \"---------------------\\n\"\n","    \"Query: {query_str}\\n\"\n","    \"Answer: \"\n",")"],"metadata":{"id":"GP_aJw7Av-1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from llama_index.core.response.notebook_utils import display_source_node\n","from llama_index.core.schema import ImageNode\n","\n","def retrieve(retriever_engine, query_str):\n","    retrieval_results = retriever_engine.retrieve(query_str)\n","\n","    retrieved_image = []\n","    retrieved_text = []\n","    for res_node in retrieval_results:\n","        if isinstance(res_node.node, ImageNode):\n","            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n","        else:\n","            display_source_node(res_node, source_length=200)\n","            retrieved_text.append(res_node.text)\n","\n","    return retrieved_image, retrieved_text"],"metadata":{"id":"wJOWM6wdwh46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n","\n","img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n","image_documents = SimpleDirectoryReader(\n","    input_dir=output_folder, input_files=img\n",").load_data()\n","context_str = \"\".join(txt)\n","plot_images(img)"],"metadata":{"id":"x0rC0OnKwmz1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n","\n","openai_mm_llm = OpenAIMultiModal(\n","    model=\"gpt-4-vision-preview\", max_new_tokens=1500\n",")\n","\n","\n","response_1 = openai_mm_llm.complete(\n","    prompt=qa_tmpl_str.format(\n","        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n","    ),\n","    image_documents=image_documents,\n",")\n","\n","pprint(response_1.text)"],"metadata":{"id":"B6NoQ2diwrtz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WDnH8Herw3gM"},"execution_count":null,"outputs":[]}]}