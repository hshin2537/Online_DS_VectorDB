{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from image_utils import crop_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clothes_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bbox'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(string, encap_type=\"()\"):\n",
    "    return [int(num) for num in string.strip(encap_type).split(', ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 읽을 때, pandas dataframe에서 list가 아닌 string 값으로 인식하기 때문에 변환 필요\n",
    "df['bbox'] = [listify(i) for i in df['bbox']]\n",
    "df['bbox_big'] = [listify(i) for i in df['bbox_big']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차 - CLIP embeddings\n",
    "\n",
    "- Local에 각 상품별 cropped image 저장\n",
    "- CLIP embeddings 생성\n",
    "\n",
    "## 1. Bounding box EDA\n",
    "\n",
    "## 2. Bounding box를 기준으로 각 eneity를 crop\n",
    "\n",
    "## 3. Cropping 된 이미지를 각 항목에 따라 resize 후 로컬에 저장\n",
    "\n",
    "## 4. CLIP을 활용한 embedding\n",
    "\n",
    "- Fine-tuned CLIP\n",
    "- 하나의 embedding space에 표현된 Text & image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bounding box EDA\n",
    "- 각 이미지에 들어있는 상품들의 '크기'는?\n",
    "- 유사도는 이미지의 크기에도 영향을 받기 때문에 중요한 요인 중 하나\n",
    "- 따라서 한 카테고리 내에 속하는 이미지들은 모두 동일한 크기로 표현되는 것이 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = crop_bbox(img, df['bbox'][218])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/007e66e7c2864eb3c1ef95cd3ab52687.jpg\")\n",
    "cropped = crop_bbox(img, df['bbox'][223])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in df['supercategory'].unique():\n",
    "    tmp = df.loc[df['supercategory']==cat]\n",
    "    print(cat)\n",
    "    print(tmp['name'].unique())\n",
    "    print(\"Area : {}, width : {}, height : {}\".format(np.median(tmp['area']), np.median(tmp['width']), np.median(tmp['height'])))\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 카테고리 별로 이미지들의 특징을 갖고 있음\n",
    "- lower body는 가로 평균 410, 세로 540\n",
    "- upper body는 lower body보다 세로 비율이 더 길다\n",
    "- wholebody는 그보다 세로 비율이 더 길다\n",
    "- waist는 기로가 세로보다 더 길다\n",
    "- arms and hands는 가로 세로 비율이 비슷하며, 전체적으로 작음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bounding box를 기준으로 각 eneity를 crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = {\"lowerbody\":[420, 540],\n",
    "        \"upperbody\":[500, 700],\n",
    "        \"wholebody\":[480, 880],\n",
    "        \"legs and feet\":[100, 150],\n",
    "        \"head\":[150, 100],\n",
    "        \"others\":[200, 350],\n",
    "        \"waist\":[200, 100],\n",
    "        \"arms and hands\":[75, 75],\n",
    "        \"neck\":[120, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/00000663ed1ff0c4e0132b9b9ac53f6e.jpg\")\n",
    "cropped = crop_bbox(img, df['bbox_big'][0])\n",
    "cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "def resize_img(image, standard_size, category):\n",
    "    w, h = image.size\n",
    "    img_size = w*h\n",
    "\n",
    "    new_width, new_height = standard_size[category]\n",
    "    new_size = new_width * new_height\n",
    "\n",
    "    if img_size >= new_size:\n",
    "        # For downsizing\n",
    "        downsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        return downsized_image\n",
    "    else:\n",
    "        # For upsizing\n",
    "        upsized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        upsized_image = upsized_image.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "        return upsized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_img(cropped, size, df['supercategory'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cropping 된 이미지를 각 항목에 따라 resize 후 로컬에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 40분 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"imaterialist-fashion-2020-fgvc7/train\"\n",
    "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for image_name in tqdm(df['ImageId'].unique()):\n",
    "    # 한 이미지와 관련된 dataframe\n",
    "    tmp = df.loc[df['ImageId']==image_name]\n",
    "    tmp = tmp.reset_index().rename(columns={\"index\":\"entity_id\"})\n",
    "    image = Image.open(os.path.join(base_path, image_name+\".jpg\"))\n",
    "    # 각 이미지 내에 있는 상품들을 crop -> local save\n",
    "    for idx, row in tmp.iterrows():\n",
    "        cropped_img = crop_bbox(image, row['bbox_big'])\n",
    "        resized_img = resize_img(cropped_img, size, row['supercategory'])\n",
    "        resized_img.save(os.path.join(cropped_path, image_name + \"_\" + str(row['entity_id']) + \".jpg\"))\n",
    "\n",
    "    new_df = pd.concat([new_df, tmp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"clothes_final2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"clothes_final2.csv\")\n",
    "\n",
    "new_df['bbox'] = [listify(i, \"[]\") for i in new_df['bbox']]\n",
    "new_df['bbox_big'] = [listify(i, \"[]\") for i in new_df['bbox_big']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CLIP을 활용한 embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fashion 데이터셋을 활용하여 pretrain된 CLIP 모델\n",
    "- CLIP 모델은 <이미지>-<caption> pair를 input data로 사용, 두 개를 하나의 동일한 embedding space에 구현\n",
    "- 따라서 <패션 이미지>-<패션 caption> pair를 활용하여 fine-tuned된 모델이 현재 프로젝트 목적에 적합\n",
    "- dot product를 사용하여 embedding ranking을 측정할 예정\n",
    "```json\n",
    "\"FashionCLIP performs the dot product between the input caption embedding and each image vector embedding\"\n",
    "\n",
    "\"The text used is a concatenation of the highlight (e.g., “stripes”, “long sleeves”, “Armani”) and short description (“80s styled t-shirt”)) available in the Farfetch dataset.\"\n",
    "```\n",
    "\n",
    "![Fine-tune 훈련 데이터](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-022-23052-9/MediaObjects/41598_2022_23052_Fig3_HTML.png?as=webp, \"Fine-tune 훈련 데이터\")\n",
    "\n",
    "(출처 : Contrastive language and vision learning of general fashion concepts)\n",
    "\n",
    "- hugging face : https://huggingface.co/patrickjohncyh/fashion-clip\n",
    "- paper : https://www.nature.com/articles/s41598-022-23052-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-CLIP VS CLIP 성능 차이\n",
    "\n",
    "https://www.nature.com/articles/s41598-022-23052-9/tables/1\n",
    "\n",
    "- HIT@5 = (서치 결과 top5에 있는 연관 상품의 개수) / (총 연관 상품의 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop된 이미지들의 path 불러오기\n",
    "cropped_path = \"imaterialist-fashion-2020-fgvc7/cropped_images\"\n",
    "\n",
    "images = list(os.walk(cropped_path))[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image embeddings from CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import extract_img_features\n",
    "\n",
    "img_emb = extract_img_features(img, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `01.Create_image_embeddings.py` 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text embeddings from CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "\n",
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_text_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    # convert the embeddings to numpy array\n",
    "    embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
    "    return embedding_as_np.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"imaterialist-fashion-2020-fgvc7/train/3bccf2e618d8f5f51442037ad3c8d4fb.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fashion fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "\"The text used is a concatenation of the highlight (e.g., “stripes”, “long sleeves”, “Armani”) and short description (“80s styled t-shirt”)) available in the Farfetch dataset.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb = extract_img_features(img, processor, model)\n",
    "\n",
    "sample_texts = ['tshirt', \"formal suit and tie\", \n",
    "                'a woman', \"a lion in a cage\", \"black top short sleeves\",\n",
    "                'black shirt with check patterns, topwear', 'iphone']\n",
    "\n",
    "sample_texts_emb = get_single_text_embedding(sample_texts, model, tokenizer)\n",
    "\n",
    "sims = cosine_similarity(img_emb.cpu().detach().numpy(), sample_texts_emb)\n",
    "# 앞으로는 dot product를 사용할 예정이지만, \n",
    "print(\"이미지와의 유사도\")\n",
    "for t, s in zip(sample_texts, sims[0]):\n",
    "    print(\"{} : {}\".format(t, s))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb.cpu().detach().numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dot product')\n",
    "for text, s in zip(sample_texts, sample_texts_emb):\n",
    "    sim = np.dot(img_emb.cpu().detach().numpy()[0], np.array(s))\n",
    "    print(text, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "\n",
    "with open('img_embeddings_fashion_fine_tuned.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Convert each line to a dictionary\n",
    "        embedding_dict = json.loads(line.strip())\n",
    "        \n",
    "        # Convert the list back to a NumPy array if necessary\n",
    "        for img_name, emb_list in embedding_dict.items():\n",
    "            embeddings[img_name] = np.array(emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in embeddings.items():\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
