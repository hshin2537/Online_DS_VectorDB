{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "### 1. VectorDB 구축\n",
    "\n",
    "### 2. Text search\n",
    "\n",
    "### 3. Image search\n",
    "\n",
    "### 4. Hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(\"attribute_specific.csv\")\n",
    "df = pd.read_csv(\"clothes_final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read_f = list()\n",
    "\n",
    "with open(\"upsert_vectors_fashion_fine_tuned.json\", 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        data_read_f.append(data)\n",
    "\n",
    "print(f\"Successfully read {len(data_read_f)} fashion-fine-tuned CLIP embeddings from img_embeddings_fashion_fine_tuned.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vdb_id'] = df['ImageId'].astype(str) + \"_\" + df['entity_id'].astype(str)\n",
    "df.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df_f = pd.DataFrame(data_read_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df_f.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.merge(df, upsert_df_f, left_on='vdb_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.to_csv(\"local_db.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = d['metadata'].values\n",
    "names = d['name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_new = list()\n",
    "\n",
    "for n,m in zip(names, metadata):\n",
    "    m['category'] = n\n",
    "    metadata_new.append(m)\n",
    "\n",
    "d['metadata'] = metadata_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PineconeDB에 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pinecone upsert 형식에 맞게 내용을 변환\n",
    "    - 각 카테고리별로 batch에 맞게 upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pineconeDB에 upsert!!\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "# index 개수 확인\n",
    "# index_list = pc.list_indexes().indexes\n",
    "\n",
    "# index description\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max size for an upsert request is 2MB. Recommended upsert limit is 100 vectors per request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 카테고리별로 나눠서 저장함\n",
    "# 이는 이후 index를 개별로 저장하기 위함\n",
    "\n",
    "# upsert!!\n",
    "def create_batches(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "df_categories = dict()\n",
    "\n",
    "for cat in tqdm(d['name'].unique()):\n",
    "    part_df = d.loc[d['name']==cat]\n",
    "    part_upserts = part_df[['id', 'values', 'sparse_values', 'metadata']].to_dict('records')\n",
    "    # 100개 단위로 upsert\n",
    "    df_categories[cat] = list(create_batches(part_upserts, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert 형식\n",
    "\n",
    "```json\n",
    "{\"id\" : \"0838a48a7b0bfa789a5181ab0e8f4ee2_3040\", # 이미지 파일 이름 + entity ID\n",
    " \"values\" : [-0.08405803143978119, -0.7088879346847534, ...], # CLIP embeddings\n",
    " \"sparse_values\" : {\n",
    "    \"indices\" : [1045, 1062, ...], # non-zero index\n",
    "    \"values\" : [1.3038887977600098, 0.304147332906723, ...] # non-zero values\n",
    "    },\n",
    "\"metadata\" : {\n",
    "    # 이미지 파일 path\n",
    "    \"img_path\": \"imaterialist-fashion-2020-fgvc7/cropped_images/0838a48a7b0bfa789a5181ab0e8f4ee2_3040.jpg\",\n",
    "    \"category\": \"coat\"\n",
    "} \n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cat, batches in df_categories.items():\n",
    "#     print(cat)\n",
    "#     for batch in tqdm(batches):\n",
    "#         index.upsert(vectors=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 카테고리별로 개별 index에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text to image search\n",
    "\n",
    "- CLIP embedding을 활용\n",
    "    - text와 image를 한 vector space에 함께 표현되어 있음\n",
    "    - 또한, fashion dataset에 맞도록 fine-tune되어, 일반 plain CLIP보다 현재 use case에 적합\n",
    "    - Fine-tune된 데이터 역시 옷의 다양한 attribute을 바탕으로 트레이닝 되어 다양한 (아래 데이터를 참고)\n",
    "\n",
    "![Fine-tune 훈련 데이터](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-022-23052-9/MediaObjects/41598_2022_23052_Fig3_HTML.png?as=webp, \"Fine-tune 훈련 데이터\")\n",
    "\n",
    "(출처 : Contrastive language and vision learning of general fashion concepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from image_utils import fetch_clip, draw_images\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import gen_sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "# splade = 'naver/splade-v3'\n",
    "splade_model = Splade(splade_model_id, agg='max')\n",
    "splade_model.to('cpu')  # move to GPU if possible\n",
    "splade_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor, tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_text_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "    # convert the embeddings to numpy array\n",
    "    embedding_as_np = text_embeddings.cpu().detach().numpy()\n",
    "    return embedding_as_np.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Green dress with blue dots, long sleeve\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"nike\"\n",
    "\n",
    "# vans, nike, addidas\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"shoe\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"street fashion\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Punk Fashion\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Bohemian Fashion\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"flower patterns, short sleeve\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 장점\n",
    "    - 유명 브랜드, 성별, 옷의 종류, 색갈 등의 카테고리를 매뉴얼하게 정하지 않아도 input으로 정해줄 수 있다\n",
    "\n",
    "- 한계점\n",
    "    - attribute들의 단순 결합이기 때문에 옷의 부위 별 특징을 인식하지 못 함\n",
    "        - 예) blue dots이라고 명시했지만, 파란색 드레스가 유사도에 표현됨\n",
    "    - 스트릿, 보헤미안 패션 등 추상적인 단어들은 다양한 옷들의 조합임\n",
    "    (CLIP은 <옷의 특징>-<옷의 사진> pair를 활용하여 학습. 따라서 \"스트릿패션\"과 같이 패션의 한 카테고리와 매칭이 되지 않는다.)\n",
    "\n",
    "- 극복 방안\n",
    "    - Sparse vector를 활용하여 옷의 특징에 weight를 더 주는 search\n",
    "    - 옷의 특징이 아닌, 보다 추상적인 텍스트가 들어오는 경우, database 전체를 대상으로 search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image to image search\n",
    "\n",
    "- CLIP embedding을 활용\n",
    "    - text와 image를 한 vector space에 함께 표현되어 있지만 Image-to-Image 유사도 측정도이 가능함\n",
    "    - 또한, fashion dataset에 맞도록 fine-tune되어, 일반 plain CLIP보다 현재 use case에 적합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import extract_img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test_image2.jpg\")\n",
    "\n",
    "img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "result = index.query(\n",
    "    vector=img_emb[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test_image.png\")\n",
    "\n",
    "img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "result = index.query(\n",
    "    vector=img_emb[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"shirt, blouse\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한계점\n",
    "    - 이미지에는 옷의 색, 사람들의 포즈, 빛 등 다양한 요소들이 모두 포함되어 있기 때문에 옷의 특징들만 선별하여 서치를 진행할 수 없음\n",
    "    - 즉, 옷의 디테일을 간과할 가능성이 높음\n",
    "- 극복 방안\n",
    "    - 이미지로부터 옷의 특징을 텍스트 형식으로 추출, dense vector 또는 sparse vector로 변환하여 서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hybrid search (Dense & sparse vector search)\n",
    "\n",
    "- splade를 활용하여 각 파트별 특징까지 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "sparse_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "# splade = 'naver/splade-v3'\n",
    "sparse_model = Splade(sparse_model_id, agg='max')\n",
    "sparse_model.to('cpu')  # move to GPU if possible\n",
    "sparse_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(sparse_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v-neck\n",
    "input_text = \"orange party dress with long sleeve, v neck\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "# sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
    "    # sparse_vector=sparse,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v-neck\n",
    "input_text = \"orange party dress with long sleeve, v neck\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=5,\n",
    "    filter={\"category\": {\"$eq\": \"dress\"}},\n",
    "    sparse_vector=sparse,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])\n",
    "\n",
    "[i['id'] for i in result.matches]\n",
    "\n",
    "df.loc[df['vdb_id'].isin([i['id'] for i in result.matches]), ['vdb_id', 'ImageId', 'AttributesNames', 'second_AttributesNames']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i['id'] for i in result.matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패션 스타일 관련 text field가 없기 때문에, sparse vector를 활용하더라도 큰 퍼포먼스 향상을 기대하기 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Punk Fashion\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "# sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    # sparse_vector=sparse,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Punk Fashion\"\n",
    "\n",
    "d = get_single_text_embedding(input_text, model, tokenizer)\n",
    "sparse = gen_sparse_vector(input_text, splade_model, splade_tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=d[0],\n",
    "    top_k=10,\n",
    "    sparse_vector=sparse,\n",
    "    filter={\"category\": {\"$eq\": \"top, t-shirt, sweatshirt\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활용할 수 있는 attribute의 종류\n",
    "```python\n",
    "list_of_attributes = ['main_category', 'silhouette', 'silhouette_fit', 'waistline', 'length',\n",
    "       'collar_type', 'neckline_type', 'sleeve_type', 'pocket_type',\n",
    "       'opening_type', 'non-textile material type', 'leather',\n",
    "       'textile finishing, manufacturing techniques', 'textile pattern']\n",
    "```\n",
    "<br>\n",
    "\n",
    "- attribute으로 표현할 수 있는 document의 포맷\n",
    "\n",
    "```json\n",
    "silhouette_name : symmetrical,\n",
    "collar_type_name : shirt (collar),\n",
    "opening_type_name : single breasted,\n",
    "non-textile material type_name : no non-textile material,\n",
    "textile finishing, manufacturing techniques_name : no special manufacturing technique,\n",
    "textile pattern_name : plain (pattern)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test_image4.png\")\n",
    "\n",
    "img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "result = index.query(\n",
    "    vector=img_emb,\n",
    "    top_k=5,  # how many results to return\n",
    "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"test_images/test_image4.png\")\n",
    "\n",
    "img_emb = extract_img_features(image, processor, model).tolist()\n",
    "\n",
    "sparse_vector = gen_sparse_vector(\"suede jacket\", sparse_model, splade_tokenizer)\n",
    "\n",
    "result = index.query(\n",
    "    vector=img_emb,\n",
    "    sparse_vector=sparse_vector,\n",
    "    top_k=5,  # how many results to return\n",
    "    filter={\"category\": {\"$eq\": \"jacket\"}},\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "paths = [i['metadata']['img_path'] for i in result.matches]\n",
    "\n",
    "draw_images([Image.open(i) for i in paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
