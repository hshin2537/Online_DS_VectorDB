{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "\n",
    "#### 1. 이미지 서치와 동일한 방식으로 검색 결과 제공\n",
    "#### 2. 결과에서 추가적으로 input text를 활용한 rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import fetch_clip, draw_images, extract_img_features\n",
    "\n",
    "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "splade_model = Splade(splade_model_id, agg='max')\n",
    "splade_model.to('cpu')\n",
    "splade_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "\n",
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "yolo_feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "yolo_model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 미리 선정된 prediction labels\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import fashion_query_transformer, clothes_detector, text_search, gen_sparse_vector, describe_clothes, additional_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db = pd.read_csv(\"local_db.csv\")\n",
    "local_db['values'] = local_db['values'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and text input <br>: Item level sequential search (text embeddings + image embeddings) -> text search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 제공되는 이미지를 기준으로 search를 하여 유관한 아이템을 N개 서치\n",
    "2. fashion style을 명시한 text search를 활용하여 rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"I want the clothes to be more casual and easy to wear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = 'test_images/test_image7.jpg'\n",
    "\n",
    "image = Image.open(open(IMAGE_PATH, \"rb\"))\n",
    "image = fix_channels(ToTensor()(image))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Image only search\n",
    "\n",
    "- 이미지와 유관한 아이템 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import clothes_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_items = clothes_detector(image, yolo_feature_extractor, yolo_model, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "\n",
    "for i, img in cropped_items.items():\n",
    "    print(i)\n",
    "    desc = describe_clothes(img, i, openai.api_key)\n",
    "    descriptions[i] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in descriptions.items():\n",
    "    print(i)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = fashion_query_transformer(str(descriptions))\n",
    "text_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100, hybrid=False)\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = dict()\n",
    "for k,v in results.items():\n",
    "    paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "# 이미지들 show\n",
    "for k,v in paths.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v[:10]]) # 10개씩만 display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking을 위한 충분한 후보군을 제공하기 위해 20를 retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = additional_search(local_db, cropped_items, results, clip_processor, clip_model, clip_tokenizer, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in final_results.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Reranking using text embeddings\n",
    "\n",
    "- Text에서 묘사된 옷의 \"방향성\"을 활용하여 Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 먼저, 텍스트가 구체적인 아이템을 언급하지 않는지 판단\n",
    "text_result = fashion_query_transformer(text_input)\n",
    "text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import get_top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clothes_type' not in text_result['items'][0].keys():\n",
    "    new_results = list()\n",
    "\n",
    "    for k,v in final_results.items():\n",
    "        # file_name을 다시 가져온다\n",
    "        ids = [os.path.splitext(os.path.basename(i))[0] for i in v]\n",
    "        tmp = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
    "\n",
    "        r = get_top_indices(tmp, text_result['items'][0]['refined_text'], k, clip_processor, clip_model, clip_tokenizer, 5, type='text')\n",
    "        new_results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_result = dict()\n",
    "\n",
    "for search_result in new_results:\n",
    "    category = list(search_result.keys())[0]\n",
    "    paths = list(search_result.values())[0]\n",
    "\n",
    "    full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
    "    refined_result[category] = full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in refined_result.items():\n",
    "    print(k)\n",
    "    draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
