{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 필요한 모델 및 function 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pineconeDB에 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"74e30e50-02fa-4e55-9bff-affa6a3817a0\")\n",
    "\n",
    "index = pc.Index(\"fastcampus\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 & 텍스트 dense vector 생성을 위한 CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import fetch_clip, draw_images, extract_img_features\n",
    "\n",
    "clip_model, clip_processor, clip_tokenizer = fetch_clip(model_name=\"patrickjohncyh/fashion-clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text sparse vector 생성을 위한 SPLADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splade.splade.models.transformer_rep import Splade\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "splade_model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "splade_model = Splade(splade_model_id, agg='max')\n",
    "splade_model.to('cpu')\n",
    "splade_model.eval()\n",
    "\n",
    "splade_tokenizer = AutoTokenizer.from_pretrained(splade_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 search methods를 구현하기 위한 function들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import fashion_query_transformer, clothes_detector, text_search, gen_sparse_vector, describe_clothes, additional_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 단계 search를 위한 local_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db = pd.read_csv(\"local_db.csv\")\n",
    "local_db['values'] = local_db['values'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image detection을 위한 YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_utils import fix_channels, visualize_predictions, rescale_bboxes, plot_results, box_cxcywh_to_xyxy\n",
    "from transformers import YolosFeatureExtractor, YolosForObjectDetection\n",
    "\n",
    "MODEL_NAME = \"valentinafeve/yolos-fashionpedia\"\n",
    "\n",
    "yolo_feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n",
    "yolo_model = YolosForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 미리 선정된 prediction labels\n",
    "cats = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai\n",
    "os.environ['OPENAI_API_KEY']= \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 인풋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search_utils import fashion_query_transformer, clothes_detector, get_top_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text input only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"a black cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gateway\n",
    "text_query = fashion_query_transformer(text_input)\n",
    "print(\"### Result from the text_input gateway : \\n{}\".format(text_query))\n",
    "\n",
    "if text_query:\n",
    "    print(\"Searching ...\")\n",
    "    # text search\n",
    "    result = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=True)\n",
    "\n",
    "    # 이미지들의 path 가져오기\n",
    "    paths = dict()\n",
    "    for k,v in result.items():\n",
    "        paths[k] = [i['metadata']['img_path'] for i in v['matches']]\n",
    "\n",
    "    # 이미지들 show\n",
    "    for k,v in paths.items():\n",
    "        print(k)\n",
    "        draw_images([Image.open(i) for i in v])\n",
    "else:\n",
    "    print(\"패션과 무관한 텍스트 입니다. 다시 입력하세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def text_search(index, items_dict, model, tokenizer, splade_model, splade_tokenizer, top_k=10, hybrid=False):\n",
    "    search_results = dict()\n",
    "    for item in items_dict['items']:\n",
    "        text_emb = get_single_text_embedding(item['refined_text'], model, tokenizer)\n",
    "        if hybrid:\n",
    "            sparse_vector = gen_sparse_vector(item['refined_text'], splade_model, splade_tokenizer)\n",
    "        else:\n",
    "            sparse_vector=None\n",
    "        \n",
    "        if 'clothes_type' in list(item.keys()):\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            filter={\"category\": {\"$eq\": item['clothes_type']}},\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results[item['clothes_type']] = search_result\n",
    "        else:\n",
    "            search_result = index.query(\n",
    "                            vector=text_emb[0],\n",
    "                            sparse_vector=sparse_vector,\n",
    "                            top_k=top_k,\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "            search_results['all'] = search_result\n",
    "    return search_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image input only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"test_images/test_image8.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "\n",
    "image = fix_channels(ToTensor()(image))\n",
    "# object detections\n",
    "print(\"Detecting items from the image.\")\n",
    "cropped_images = clothes_detector(image, yolo_feature_extractor, yolo_model, thresh=0.7)\n",
    "\n",
    "if len(cropped_images.keys())==0:\n",
    "    print(\"Nothing detected from the image\")\n",
    "else:\n",
    "    print(\"Detected \", cropped_images.keys())\n",
    "    \n",
    "    # describe the labels I have found\n",
    "    descriptions = dict()\n",
    "\n",
    "    print(\"Start creating descriptions for each item.\")\n",
    "    for i, img in cropped_images.items():\n",
    "        print(\"Created descriptions for {}\".format(i))\n",
    "        desc = describe_clothes(img, i, openai.api_key)\n",
    "        descriptions[i] = desc\n",
    "    print(\"\\nTransform the descriptions into structured query.\")\n",
    "    text_query = fashion_query_transformer(str(descriptions))\n",
    "    print(text_query)\n",
    "    results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=100)\n",
    "    print(\"\\nRetrieved 100 images based on text search\")\n",
    "\n",
    "    print(\"\\nConducting additional search using the input images\")\n",
    "\n",
    "    results2 = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer, 10)\n",
    "\n",
    "    for k,v in results2.items():\n",
    "        print(k)\n",
    "        draw_images([Image.open(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text and Image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존의 이미지와 지나치게 다른 fashion style을 입력시, 원하는 결과가 나오지 않을 가능성이 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = \"softer and more comfortable material\"\n",
    "image_path = \"test_images/test_image2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = fashion_query_transformer(text_input)\n",
    "print(\"### Result from the text_input gateway : \\n{}\".format(text_query))\n",
    "\n",
    "# 패션과 유관한 쿼리\n",
    "\n",
    "if 'clothes_type' in text_query['items'][0].keys():\n",
    "    print(\"구체적인 아이템 보다는, 원하는 패션 스타일을 입력해주세요\")\n",
    "elif text_query:\n",
    "    image = Image.open(image_path)\n",
    "    image = fix_channels(ToTensor()(image))\n",
    "    # object detections\n",
    "    print(\"Detecting items from the image.\")\n",
    "    cropped_images = clothes_detector(image, yolo_feature_extractor, yolo_model)\n",
    "\n",
    "    if len(cropped_images.keys())==0:\n",
    "        print(\"Nothing detected from the image\")\n",
    "    else:\n",
    "        print(\"Detected \", cropped_images.keys())\n",
    "        print(\"-\"*10, \"Start image only search\", \"-\"*10)\n",
    "        \n",
    "        # describe the labels I have found\n",
    "        descriptions = dict()\n",
    "\n",
    "        print(\"Start creating descriptions for each item.\")\n",
    "        for i, img in cropped_images.items():\n",
    "            print(\"Created descriptions for {}\".format(i))\n",
    "            desc = describe_clothes(img, i, openai.api_key)\n",
    "            descriptions[i] = desc\n",
    "        print(\"\\nTransform the descriptions into structured query.\")\n",
    "        text_query = fashion_query_transformer(str(descriptions))\n",
    "        print(text_query)\n",
    "        results = text_search(index, text_query, clip_model, clip_tokenizer, splade_model, splade_tokenizer, top_k=200)\n",
    "        print(\"\\nRetrieved 200 images based on text search\")\n",
    "\n",
    "        print(\"\\nConducting additional search using the input images\")\n",
    "\n",
    "        results2 = additional_search(local_db, cropped_images, results, clip_processor, clip_model, clip_tokenizer, 100)\n",
    "        print(\"\\nRetrieved 100 items each, from sequential image search\")\n",
    "\n",
    "        print(\"-\"*10, \"Start reranking the results based on user input text\", \"-\"*10)\n",
    "        # 텍스트 검색\n",
    "        new_results = list()\n",
    "\n",
    "        for k,v in results2.items():\n",
    "            ids = [os.path.splitext(os.path.basename(i))[0] for i in v]\n",
    "            tmp = local_db.loc[local_db['vdb_id'].isin(ids)]\n",
    "\n",
    "            r = get_top_indices(tmp, text_query['items'][0]['refined_text'], k, clip_processor, clip_model, clip_tokenizer, 10, type='text')\n",
    "            new_results.append(r)\n",
    "\n",
    "        refined_result = dict()\n",
    "\n",
    "        for search_result in new_results:\n",
    "            category = list(search_result.keys())[0]\n",
    "            paths = list(search_result.values())[0]\n",
    "\n",
    "            full_paths = [os.path.join(\"imaterialist-fashion-2020-fgvc7\", \"cropped_images\", i+\".jpg\") for i in paths]\n",
    "            refined_result[category] = full_paths\n",
    "        for k,v in refined_result.items():\n",
    "            print(k)\n",
    "            draw_images([Image.open(i) for i in v])\n",
    "else:\n",
    "    print(\"패션과 무관한 텍스트 입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastcampus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
